from typing import Dict, Tuple, Optional

from core.config import settings
from core.llm import call_llm
from core.utils import logger, safe_execute
from core.rag import build_prompt_context
from db.duckdb_client import DuckDBClient


# -------------------------------
# BUILD PROMPT
# -------------------------------

def _build_nl_to_sql_prompt(query: str, context: str) -> str:
    """
    Construct the LLM prompt for converting a user question into SQL.
    This prompt is the MOST CRITICAL part — it guides the LLM toward:
    - No hallucinations
    - Using only recommended tables/columns
    - Producing validated DuckDB SQL
    - No explanation, no chain-of-thought, SQL only
    """
    return f"""
        You are an expert SQL generator for DuckDB.
        Your job is to translate the user question into a **single SQL query**.

        STRICT RULES:
        1. Use ONLY the tables and columns listed in the schema below.
        2. If the schema does not contain a column, DO NOT invent it.
        3. Prefer fully-qualified names: table.column
        4. Write correct DuckDB SQL.
        5. NO natural language explanation. Return SQL only.
        6. Do not wrap SQL in backticks.
        7. Do not include comments inside the SQL.
        8. Use correct table names EXACTLY as shown.

        ================= CONTEXT (RAG) =================
        {context}
        =================================================

        USER QUESTION:
        {query}

        Now generate ONLY the SQL query:
    """.strip()


# -------------------------------
# SELECT WHICH LLM TO USE
# -------------------------------

def _select_llm_client():
    """
    Returns an LLM client (OpenAI or Gemini) depending on settings.LLM_PROVIDER.
    This allows switching between providers without code changes.
    """
    provider = settings.LLM_PROVIDER.lower()

    if provider == "openai":
        from openai import OpenAI
        return OpenAI(api_key=settings.OPENAI_API_KEY)

    # elif provider == "gemini":
    #     # Using Google Generative AI client
    #     from google import generativeai as genai
    #     genai.configure(api_key=settings.GEMINI_API_KEY)
    #     return genai

    else:
        raise ValueError(f"Unsupported LLM provider: {settings.LLM_PROVIDER}")


# -------------------------------
# EXECUTE MODEL CALL
# -------------------------------

def _call_llm(prompt: str) -> str:
    llm = _select_llm_client()
    provider = settings.LLM_PROVIDER.lower()
    logger.info(f"Calling LLM provider: {provider}")

    if provider == "openai":
        response = llm.chat.completions.create(
            model=settings.LLM_MODEL_NAME,
            messages=[{"role": "user", "content": prompt}],
            temperature=settings.LLM_TEMPERATURE,
            max_tokens=settings.LLM_MAX_TOKENS
        )
        logger.info(f"Token usage: prompt={response.usage.prompt_tokens}, "
                    f"completion={response.usage.completion_tokens}, "
                    f"total={response.usage.total_tokens}")

        return response.choices[0].message.content.strip()

    elif provider == "gemini":
        model = llm.GenerativeModel(settings.LLM_MODEL_NAME)
        response = model.generate_content(prompt)

        # Gemini token usage
        usage = response.usage_metadata
        logger.info(f"Token usage: input={usage.prompt_token_count}, "
                    f"output={usage.candidates_token_count}, "
                    f"total={usage.total_token_count}")

        return response.text.strip()


# -------------------------------
# NORMALIZE SQL OUTPUT
# -------------------------------

def _clean_sql(sql_raw: str) -> str:
    """
    Remove backticks, code fences, and stray text.
    Keep only pure SQL.
    """
    if not sql_raw:
        return ""

    # Remove common wrappers
    for bad in ["```sql", "```", "`"]:
        sql_raw = sql_raw.replace(bad, "")

    return sql_raw.strip()


# -------------------------------
# MAIN PUBLIC FUNCTION
# -------------------------------

@safe_execute
def generate_sql_from_question(query: str, top_k: int = 8) -> Tuple[str, Dict]:
    """
    High-level entry point for generating SQL.

    Returns:
        sql_text: the SQL generated by LLM
        schema_used: the structured schema dict returned by RAG

    Notes:
    - This does NOT execute the SQL. Execution happens later.
    - The validator agent should check correctness.
    """
    logger.info(f"NL→SQL Agent processing question: {query!r}")

    # 1. Build RAG context (schema + recommended columns)
    context_str, schema_struct = build_prompt_context(query, top_k=top_k)

    if not context_str:
        logger.warn("RAG context is empty — proceeding with fallback minimal prompt.")
        context_str = "# No metadata found — LLM must ask for clarifications."

    # 2. Construct full prompt for LLM
    prompt = _build_nl_to_sql_prompt(query, context_str)

    logger.debug("Final NL→SQL prompt constructed.")

    # 3. Call LLM
    sql_raw = call_llm(prompt)

    if not sql_raw:
        logger.error("LLM returned empty SQL output.")
        return "", schema_struct

    # 4. Normalize SQL output
    cleaned_sql = _clean_sql(sql_raw)
    logger.success(f"Generated SQL:\n{cleaned_sql}")

    return cleaned_sql, schema_struct
